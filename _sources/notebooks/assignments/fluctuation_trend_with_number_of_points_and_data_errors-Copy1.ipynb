{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follow-up: fluctuation trends with # of points and data errors\n",
    "\n",
    "This is a follow-up to `Assignment: Follow-ups to Parameter Estimation notebooks`, which focuses on the trends of fluctuations and how to visualize them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Parameter estimation example: fitting a straight line\n",
    "\n",
    "\n",
    "&nbsp; 2.&nbsp;&nbsp; Do exercise 3: \"Change the random number seed to get different results and comment on how the maximum likelihood results fluctuate. How are size of the fluctuations related to the number of data points $N$ and the data error standard deviation $dy$?  (Try changing them!)\"\n",
    "<br><br>**As the number of data points $N$ increases, the size of the fluctuations decrease as square root N which makes sense due to the central limit theorem. As the data error standard deviation increases, the size of the fluctuations increases linearly with N.**<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn; seaborn.set('talk') # for plot formatting\n",
    "from scipy import optimize\n",
    "\n",
    "def make_data(intercept, slope, N=20, dy=5, rseed=None):\n",
    "    \"\"\"Given a straight line defined by intercept and slope:\n",
    "          y = slope * x + intercept\n",
    "       generate N points randomly spaced points from x=0 to x=100\n",
    "       with Gaussian (i.e., normal) error with mean zero and standard\n",
    "       deviation dy.\n",
    "       \n",
    "       Return the x and y arrays and an array of standard deviations.\n",
    "    \"\"\"\n",
    "    rand = np.random.RandomState(rseed) \n",
    "    x = 100 * rand.rand(N)  # choose the x values randomly in [0,100]\n",
    "    y = intercept + slope * x  # This is the y value without noise\n",
    "    y += dy * rand.randn(N)    # Add in Gaussian noise\n",
    "    return x, y, dy * np.ones_like(x)  # return coordinates and error bars\n",
    "\n",
    "def log_likelihood(theta, x, y, dy):\n",
    "    y_model = theta[0] + theta[1]*x\n",
    "    return -0.5*np.sum(np.log(2*np.pi*dy**2)+ (y - y_model)**2/dy**2)\n",
    "\n",
    "def minfunc(theta, x, y, dy):\n",
    "    \"\"\"\n",
    "    Function to be minimized: minus the logarithm of the likelihood.\n",
    "    \"\"\"\n",
    "    return -log_likelihood(theta, x, y, dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First make tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = 25.   # true intercept (called b elsewhere)\n",
    "slope = 0.5       # true slope (called m elsewhere)\n",
    "theta_true = [intercept, slope]  # put parameters in a true theta vector\n",
    "\n",
    "iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix dy and vary Npts geometrically\n",
    "dy_data = 5\n",
    "for Npts in [20, 80, 320]:\n",
    "    print(f'N = {Npts}, dy = {dy_data}')\n",
    "    print('          intercept   slope')\n",
    "    print(f'true:       {intercept:.3f}    {slope:.3f}')\n",
    "    \n",
    "    for i in np.arange(iterations):        \n",
    "        x, y, dy = make_data(*theta_true, N=Npts, dy=dy_data, rseed=None)\n",
    "        result = optimize.minimize(minfunc, x0=[0, 0], args=(x, y, dy))\n",
    "        intercept_fit, slope_fit = result.x\n",
    "    \n",
    "        print(f'dataset {i}:  {intercept_fit:.3f}    {slope_fit:.3f}')\n",
    "    print('------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Npts and vary dy geometically\n",
    "Npts = 80\n",
    "for dy_data in [1, 5, 25]:\n",
    "    print(f'N = {Npts}, dy = {dy_data}')\n",
    "    print('          intercept   slope')\n",
    "    print(f'true:       {intercept:.3f}    {slope:.3f}')\n",
    "    \n",
    "    for i in np.arange(iterations):        \n",
    "        x, y, dy = make_data(*theta_true, N=Npts, dy=dy_data, rseed=None)\n",
    "        result = optimize.minimize(minfunc, x0=[0, 0], args=(x, y, dy))\n",
    "        intercept_fit, slope_fit = result.x\n",
    "    \n",
    "        print(f'dataset {i}:  {intercept_fit:.3f}    {slope_fit:.3f}')\n",
    "    print('------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now make a function for rerunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_of_fit_data(Npts, dy_data, iterations, theta_true=theta_true):\n",
    "    \"\"\"Calculate the standard deviations of the slope and intercept \n",
    "       for a given number of iterations\n",
    "    \"\"\" \n",
    "    intercept_fits = np.zeros(iterations)\n",
    "    slope_fits = np.zeros(iterations)\n",
    "\n",
    "    for j in np.arange(iterations):        \n",
    "        x, y, dy = make_data(*theta_true, N=Npts, dy=dy_data, rseed=None)\n",
    "        result = optimize.minimize(minfunc, x0=[0, 0], args=(x, y, dy))\n",
    "        intercept_fits[j], slope_fits[j] = result.x\n",
    "        \n",
    "    return intercept_fits.std(), slope_fits.std()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_of_fit_data(20, 5, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_of_fit_data(80, 5, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_of_fit_data(320, 5, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now make linear and log-log plots\n",
    "\n",
    "Which is better? How do you read a power law from a log-log plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Npts_array = [20 * 2**i for i in range(10)]\n",
    "Npts_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix dy and vary Npts geometrically\n",
    "dy_data = 5\n",
    "\n",
    "Npts_array = [20 * 2**j for j in range(10)]\n",
    "intercept_std_array = np.zeros(len(Npts_array))\n",
    "slope_std_array = np.zeros(len(Npts_array))\n",
    "\n",
    "iterations = 20\n",
    "for i, Npts in enumerate(Npts_array):\n",
    "    intercept_std_array[i], slope_std_array[i] = std_of_fit_data(Npts, dy_data, iterations)   \n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "axes[0,0].set_title(r'intercept fluctuations vs $N$')\n",
    "axes[0,0].set_xlabel(r'$N$')\n",
    "axes[0,0].plot(Npts_array, intercept_std_array)\n",
    "\n",
    "axes[0,1].set_title(r'intercept fluctuations vs $N$')\n",
    "axes[0,1].set_xlabel(r'$N$')\n",
    "axes[0,1].loglog(Npts_array, intercept_std_array)\n",
    "axes[0,1].set_xlim(10,1e4)\n",
    "axes[0,1].set_ylim(.01,10)\n",
    "axes[0,1].set_aspect('equal')\n",
    "\n",
    "axes[1,0].set_title('slope fluctuations vs $N$')\n",
    "axes[1,0].set_xlabel(r'$N$')\n",
    "axes[1,0].plot(Npts_array, slope_std_array)\n",
    "\n",
    "axes[1,1].set_title('slope fluctuations vs $N$')\n",
    "axes[1,1].set_xlabel(r'$N$')\n",
    "axes[1,1].loglog(Npts_array, slope_std_array)\n",
    "axes[1,1].set_xlim(10,1e4)\n",
    "axes[1,1].set_ylim(1e-3,1e-1)\n",
    "axes[1,1].set_aspect('equal')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Npts and vary dy geometrically\n",
    "Npts = 20\n",
    "\n",
    "dy_array = [1 * 2**j for j in range(10)]\n",
    "intercept_std_array = np.zeros(len(dy_array))\n",
    "slope_std_array = np.zeros(len(dy_array))\n",
    "\n",
    "iterations = 20\n",
    "for i, dy_data in enumerate(dy_array):\n",
    "    intercept_std_array[i], slope_std_array[i] = std_of_fit_data(Npts, dy_data, iterations)   \n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "axes[0,0].set_title(r'intercept fluctuations vs $dy$')\n",
    "axes[0,0].set_xlabel(r'$dy$')\n",
    "axes[0,0].plot(Npts_array, intercept_std_array)\n",
    "\n",
    "axes[0,1].set_title(r'intercept fluctuations vs $dy$')\n",
    "axes[0,1].set_xlabel(r'$dy$')\n",
    "axes[0,1].loglog(Npts_array, intercept_std_array)\n",
    "axes[0,1].set_xlim(1e1,1e4)\n",
    "axes[0,1].set_ylim(.1,1000)\n",
    "#axes[0,1].set_aspect('equal')\n",
    "\n",
    "axes[1,0].set_title('slope fluctuations vs $dy$')\n",
    "axes[1,0].set_xlabel(r'$dy$')\n",
    "axes[1,0].plot(Npts_array, slope_std_array)\n",
    "\n",
    "axes[1,1].set_title('slope fluctuations vs $dy$')\n",
    "axes[1,1].set_xlabel(r'$dy$')\n",
    "axes[1,1].loglog(Npts_array, slope_std_array)\n",
    "axes[0,1].set_xlim(1e1,1e4)\n",
    "axes[1,1].set_ylim(1e-3,1e+1)\n",
    "#axes[1,1].set_aspect('equal')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In both sets of joint posterior graphs, are the slope and intercept correlated?  How do you know? Does it make sense?\n",
    "<br><br>\n",
    "\n",
    "1. For the first set of data, answer the question: \"What do you conclude about how the form of the prior affects the final posterior in this case?\"\n",
    "<br><br>\n",
    "\n",
    "1. For the second set of data, answer the question: \"Why in this case does the form of the prior have a clear effect?\"  You should consider both the size of the error bars and the number of data points (try changing them to explore the impact).\n",
    "<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
