
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>7.2. Learning from data: Gaussian processes &#8212; Learning from data</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"N": ["\\mathbb{N}"], "Z": ["\\mathbb{Z}"], "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"], "muvec": ["\\boldsymbol{\\mu}"], "phivec": ["\\boldsymbol{\\phi}"], "sigmavec": ["\\boldsymbol{\\sigma}"], "Sigmavec": ["\\boldsymbol{\\Sigma}"], "thetavec": ["\\boldsymbol{\\theta}"], "thetavechat": ["\\widehat\\thetavec"], "avec": ["\\boldsymbol{a}"], "Bvec": ["\\boldsymbol{B}"], "xvec": ["\\boldsymbol{x}"], "yvec": ["\\boldsymbol{y}"], "Lra": ["\\Longrightarrow"], "abar": ["\\overline a"], "Xbar": ["\\overline X"], "alphahat": ["\\widehat\\alpha"], "Hhat": ["\\hat H"], "yth": ["y_{\\text{th}}"], "yexp": ["y_{\\text{exp}}"], "ym": ["y_{\\text{m}}"]}}})</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="7.3. Exercise: Gaussian Process models with GPy" href="Gaussian_processes_exercises.html" />
    <link rel="prev" title="7.1. Gaussian processes demonstration" href="demo-GaussianProcesses.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/8820_icon.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning from data</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../about.html">
   About this Jupyter Book
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Course overview
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/Course/overview.html">
   Objectives
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Topics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Basics/basics.html">
   1. Basics of Bayesian statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Basics/lecture_01.html">
     1.1. Lecture 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/Exploring_pdfs.html">
     1.2. Exploring PDFs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/simple_sum_product_rule.html">
     1.3. Checking the sum and product rules, and their consequences
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Basics/lecture_02.html">
     1.4. Lecture 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/Bayesian_updating_coinflip_interactive.html">
     1.5. Interactive Bayesian updating: coin flipping example
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/medical_example_by_Bayes.html">
     1.6. Standard medical example by applying Bayesian rules of probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/radioactive_lighthouse_exercise.html">
     1.7. Radioactive lighthouse problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Basics/lecture_03.html">
     1.8. Lecture 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Parameter_estimation/param_est.html">
   2. Bayesian parameter estimation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Parameter_estimation/lecture_04.html">
     2.1. Lecture 4: Parameter estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/parameter_estimation_Gaussian_noise.html">
     2.2. Parameter estimation example: Gaussian noise and averages
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/Assignment_extending_radioactive_lighthouse.html">
     2.3. Assignment: 2D radioactive lighthouse location using MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Parameter_estimation/lecture_05.html">
     2.4. Lecture 5
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/parameter_estimation_fitting_straight_line_I.html">
     2.5. Parameter estimation example: fitting a straight line
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/demo-ModelValidation.html">
     2.6. Linear Regression and Model Validation demonstration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Parameter_estimation/lecture_06.html">
     2.7. Lecture 6
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/amplitude_in_presence_of_background.html">
     2.8. Amplitude of a signal in the presence of background
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/Assignment_parameter_estimation_followups.html">
     2.9. Assignment: Follow-ups to Parameter Estimation notebooks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/exercise_LinearRegression.html">
     2.10. Linear Regression exercise
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/linear_algebra_games_I.html">
     2.11. Linear algebra games including SVD for PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/fluctuation_trend_with_number_of_points_and_data_errors.html">
     2.12. Follow-up: fluctuation trends with # of points and data errors
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/MCMC_sampling_I/MCMC_sampling_I.html">
   3. MCMC sampling I
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/MCMC_sampling_I/lecture_07.html">
     3.1. Lecture 7
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_I/Metropolis_Poisson_example.html">
     3.2. Metropolis-Hasting MCMC sampling of a Poisson distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/MCMC_sampling_I/lecture_08.html">
     3.3. Lecture 8
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_I/MCMC-random-walk-and-sampling.html">
     3.4. Exercise: Random walk
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Why_Bayes_is_better/bayes_is_better.html">
   4. Why Bayes is better
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Why_Bayes_is_better/lecture_09.html">
     4.1. Lecture 9
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/bayes_billiard.html">
     4.2. A Bayesian Billiard game
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Why_Bayes_is_better/lecture_10.html">
     4.3. Lecture 10
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/parameter_estimation_fitting_straight_line_II.html">
     4.4. Parameter estimation example: fitting a straight line II
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Why_Bayes_is_better/lecture_11.html">
     4.5. Lecture 11
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/error_propagation_to_functions_of_uncertain_parameters.html">
     4.6. Error propagation: Example 3.6.2 in Sivia
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/visualization_of_CLT.html">
     4.7. Visualization of the Central Limit Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/correlation_intuition.html">
     4.8. Building intuition about correlations (and a bit of Python linear algebra)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Why_Bayes_is_better/lecture_12.html">
     4.9. Lecture 12
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_I/MCMC-diagnostics.html">
     4.10. Overview: MCMC Diagnostics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Why_Bayes_is_better/lecture_13.html">
     4.12. Lecture 13
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/dealing_with_outliers.html">
     4.13. Dealing with outliers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Model_selection/model_selection.html">
   5. Model selection
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Model_selection/lecture_14.html">
     5.1. Lecture 14
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Model_selection/lecture_15.html">
     5.2. Lecture 15
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Model_selection/Evidence_for_model_EFT_coefficients.html">
     5.3. Evidence calculation for EFT expansions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Model_selection/lecture_16.html">
     5.4. Lecture 16
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/MCMC_sampling_II/MCMC_sampling_II.html">
   6. MCMC sampling II
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/Liouville_theorem_visualization.html">
     6.1. Liouville Theorem Visualization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/Orbital_eqs_with_different_algorithms.html">
     6.2. Solving orbital equations with different algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/PyMC3_intro.html">
     6.4. PyMC3 Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/PyMC3_docs_getting_started.html">
     6.5. Getting started with PyMC3
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../../content/Gaussian_processes/gaussian_processes.html">
   7. Gaussian processes
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="demo-GaussianProcesses.html">
     7.1. Gaussian processes demonstration
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     7.2. Learning from data: Gaussian processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Gaussian_processes_exercises.html">
     7.3. Exercise: Gaussian Process models with GPy
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/Emulators/emulators.html">
   8. Emulators
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Maximum_entropy/max_ent.html">
   9. Assigning probabilities
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Maximum_entropy/demo-MaxEnt.html">
     9.1. Assigning probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Maximum_entropy/MaxEnt.html">
     9.2. Ignorance pdfs: Indifference and translation groups
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Maximum_entropy/Pdfs_from_MaxEnt.html">
     9.3. MaxEnt for deriving probability distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Maximum_entropy/MaxEnt_Function_Reconstruction.html">
     9.4. Maximum Entropy for reconstructing a function from its moments
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Machine_learning/machine_learning.html">
   10. Machine learning: Bayesian methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Machine_learning/Bayesian_optimization.html">
     10.1. Physics 8805
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/SVD/svd.html">
   11. PCA, SVD, and all that
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../SVD/linear_algebra_games_including_SVD.html">
     11.1. Linear algebra games including SVD for PCA
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/Model_mixing/model_mixing.html">
   12. Model mixing
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Mini-projects
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../mini-projects/mini-project_I_toy_model_of_EFT.html">
   Mini-project I: Parameter estimation for a toy model of an EFT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mini-projects/model-selection_mini-project-IIa.html">
   Mini-project IIa: Model selection basics
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reference material
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/zbibliography.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/related_topics.html">
   Related topics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/Reference/installing_anaconda.html">
   Using Anaconda
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/Reference/using_github.html">
   Using GitHub
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Reference/python_jupyter.html">
   Python and Jupyter notebooks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Reference/Jupyter_Python_intro_01.html">
     Python and Jupyter notebooks: part 01
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Reference/Jupyter_Python_intro_02.html">
     Python and Jupyter notebooks: part 02
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/jb_tests.html">
   Examples: Jupyter jb-book
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Notebook keys
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Basics/simple_sum_product_rule_KEY.html">
   Checking the sum and product rules, and their consequences
   <span style="color: red">
    Key
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Basics/medical_example_by_Bayes_KEY.html">
   Standard medical example by applying Bayesian rules of probability
   <span style="color: red">
    Key
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Basics/radioactive_lighthouse_exercise_key.html">
   Radioactive lighthouse problem
   <span style="color: red">
    Key
   </span>
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/notebooks/Gaussian_processes/GaussianProcesses.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/furnstahl/Physics-8820"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/furnstahl/Physics-8820/issues/new?title=Issue%20on%20page%20%2Fnotebooks/Gaussian_processes/GaussianProcesses.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/furnstahl/Physics-8820/main?urlpath=tree/./LectureNotes/notebooks/Gaussian_processes/GaussianProcesses.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference-using-gaussian-processes">
   Inference using Gaussian processes
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#references">
     References:
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parametric-approach">
   Parametric approach
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#proof">
     Proof
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-covariance-matrix-as-the-central-object">
     The covariance matrix as the central object
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#non-parametric-approach-mean-and-covariance-functions">
   Non-parametric approach: Mean and covariance functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stationary-kernels">
     Stationary kernels
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gp-models-for-regression">
   GP models for regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#elegant-approach-using-linear-algebra-tricks">
     Elegant approach using linear algebra tricks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimizing-the-gp-model-hyperparameters">
     Optimizing the GP model hyperparameters
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <!-- dom:TITLE: Learning from data: Gaussian processes -->
<div class="section" id="learning-from-data-gaussian-processes">
<h1><span class="section-number">7.2. </span>Learning from data: Gaussian processes<a class="headerlink" href="#learning-from-data-gaussian-processes" title="Permalink to this headline">¶</a></h1>
<!-- dom:AUTHOR: Christian Forssén at Department of Physics, Chalmers University of Technology, Sweden -->
<!-- Author: -->  
<p><strong>Christian Forssén</strong>, Department of Physics, Chalmers University of Technology, Sweden</p>
<p>Date: <strong>Oct 21, 2019</strong></p>
<p>Copyright 2018-2019, Christian Forssén. Released under CC Attribution-NonCommercial 4.0 license</p>
<div class="section" id="inference-using-gaussian-processes">
<h2>Inference using Gaussian processes<a class="headerlink" href="#inference-using-gaussian-processes" title="Permalink to this headline">¶</a></h2>
<p>Assume that there is a set of input vectors with independent, predictor, variables</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X}_N \equiv \{ \boldsymbol{x}^{(n)}\}_{n=1}^N
\]</div>
<p>and a set of target values</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{t}_N \equiv \{ t^{(n)}\}_{n=1}^N.
\]</div>
<ul class="simple">
<li><p>Note that we will use the symbol <span class="math notranslate nohighlight">\(t\)</span> to denote the target, or response, variables in the context of Gaussian Processes.</p></li>
<li><p>Furthermore, we will use the subscript <span class="math notranslate nohighlight">\(N\)</span> to denote a vector of <span class="math notranslate nohighlight">\(N\)</span> vectors (or scalars): <span class="math notranslate nohighlight">\(\boldsymbol{X}_N\)</span> (<span class="math notranslate nohighlight">\(\boldsymbol{t}_N\)</span>)</p></li>
<li><p>While a single instance <span class="math notranslate nohighlight">\(i\)</span> is denoted by a superscript: <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(i)}\)</span> (<span class="math notranslate nohighlight">\(t^{(i)}\)</span>).</p></li>
</ul>
<p>We will consider two different <em>inference problems</em>:</p>
<ol class="simple">
<li><p>The prediction of <em>new target</em> <span class="math notranslate nohighlight">\(t^{(N+1)}\)</span> given a new input <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(N+1)}\)</span></p></li>
<li><p>The inference of a <em>function</em> <span class="math notranslate nohighlight">\(y(\boldsymbol{x})\)</span> from the data.</p></li>
</ol>
<p>The former can be expressed with the pdf</p>
<div class="math notranslate nohighlight">
\[
p\left( t^{(N+1)} | \boldsymbol{t}_N, \boldsymbol{X}_{N}, \boldsymbol{x}^{(N+1)} \right)
\]</div>
<p>while the latter can be written using Bayes’ formula (in these notes we will not be including information <span class="math notranslate nohighlight">\(I\)</span> explicitly in the conditional probabilities)</p>
<div class="math notranslate nohighlight">
\[
p\left( y(\boldsymbol{x}) | \boldsymbol{t}_N, \boldsymbol{X}_N \right)
= \frac{p\left( \boldsymbol{t}_N | y(\boldsymbol{x}), \boldsymbol{X}_N \right) p \left( y(\boldsymbol{x}) \right) }
{p\left( \boldsymbol{t}_N | \boldsymbol{X}_N \right) }
\]</div>
<p>The inference of a function will obviously also allow to make predictions for new targets.
However, we will need to consider in particular the second term in the numerator, which is the <strong>prior</strong> distribution on functions assumed in the model.</p>
<ul class="simple">
<li><p>This prior is implicit in parametric models with priors on the parameters.</p></li>
<li><p>The idea of Gaussian process modeling is to put a prior directly on the <strong>space of functions</strong> without parameterizing <span class="math notranslate nohighlight">\(y(\boldsymbol{x})\)</span>.</p></li>
<li><p>A Gaussian process can be thought of as a generalization of a Gaussian distribution over a finite vector space to a <strong>function space of infinite dimension</strong>.</p></li>
<li><p>Just as a Gaussian distribution is specified by its mean and covariance matrix, a Gaussian process is specified by a <strong>mean and covariance function</strong>.</p></li>
</ul>
<p><strong>Gaussian process.</strong></p>
<p>A Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution</p>
<div class="section" id="references">
<h3>References:<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p><a class="reference external" href="http://www.gaussianprocess.org/gpml">Gaussian Processes for Machine Learning</a>, Carl Edward Rasmussen and Chris Williams, the MIT Press, 2006, <a class="reference external" href="http://www.gaussianprocess.org/gpml/chapters">online version</a>.</p></li>
<li><p><a class="reference external" href="https://sheffieldml.github.io/GPy/">GPy</a>: a Gaussian Process (GP) framework written in python, from the Sheffield machine learning group.</p></li>
</ol>
</div>
</div>
<div class="section" id="parametric-approach">
<h2>Parametric approach<a class="headerlink" href="#parametric-approach" title="Permalink to this headline">¶</a></h2>
<p>Let us express <span class="math notranslate nohighlight">\(y(\boldsymbol{x})\)</span> in terms of a model function <span class="math notranslate nohighlight">\(y(\boldsymbol{x}; \boldsymbol{\theta})\)</span> that depends on a vector of model parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p>
<p>For example, using a set of basis functions <span class="math notranslate nohighlight">\(\left\{ \phi^{(h)} (\boldsymbol{x}) \right\}_{h=1}^H\)</span> with linear weights <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_H\)</span> we have</p>
<div class="math notranslate nohighlight">
\[
y (\boldsymbol{x}, \boldsymbol{\theta}) = \sum_{h=1}^H \theta^{(h)} \phi^{(h)} (\boldsymbol{x})
\]</div>
<p><strong>Notice.</strong></p>
<p>The basis functions can be non-linear such as Gaussians (aka <em>radial basis functions</em>)</p>
<div class="math notranslate nohighlight">
\[
\phi^{(h)} (\boldsymbol{x}) = \exp \left[ -\frac{\left( \boldsymbol{x} - \boldsymbol{c}^{(h)} \right)^2}{2 (\sigma^{(h)})^2} \right].
\]</div>
<p>Still, this constitutes a linear model since <span class="math notranslate nohighlight">\(y (\boldsymbol{x}, \boldsymbol{\theta})\)</span> depends linearly on the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p>
<p>The inference of model parameters should be a well-known problem by now. We state it in terms of Bayes theorem</p>
<div class="math notranslate nohighlight">
\[
p \left( \boldsymbol{\theta} | \boldsymbol{t}_N, \boldsymbol{X}_N \right)
= \frac{ p \left( \boldsymbol{t}_N | \boldsymbol{\theta}, \boldsymbol{X}_N \right) p \left( \boldsymbol{\theta} \right)}{p \left( \boldsymbol{t}_N | \boldsymbol{X}_N \right)}
\]</div>
<p>Having solved this inference problem (e.g. by linear regression) a prediction can be made through marginalization</p>
<div class="math notranslate nohighlight">
\[
p\left( t^{(N+1)} | \boldsymbol{t}_N, \boldsymbol{X}_{N}, \boldsymbol{x}^{(N+1)} \right) 
= \int d^H \boldsymbol{\theta} 
p\left( t^{(N+1)} | \boldsymbol{\theta}, \boldsymbol{x}^{(N+1)} \right)
p \left( \boldsymbol{\theta} | \boldsymbol{t}_N, \boldsymbol{X}_N \right).
\]</div>
<p>Here it is important to note that the final answer does not make any explicit reference to our parametric representation of the unknown function <span class="math notranslate nohighlight">\(y(\boldsymbol{x})\)</span>.</p>
<p>Assuming that we have a fixed set of basis functions and Gaussian prior distributions (with zero mean) on the weights <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> we will show that:</p>
<ul class="simple">
<li><p>The joint pdf of the observed data given the model <span class="math notranslate nohighlight">\(p( \boldsymbol{t}_N |  \boldsymbol{X}_N)\)</span>, is a multivariate Gaussian with mean zero and with a covariance matrix that is determined by the basis functions.</p></li>
<li><p>This implies that the conditional distribution <span class="math notranslate nohighlight">\(p( t^{(N+1)} | \boldsymbol{t}_N, \boldsymbol{X}_{N+1})\)</span>, is also a multivariate Gaussian whose mean depends linearly on <span class="math notranslate nohighlight">\(\boldsymbol{t}_N\)</span>.</p></li>
</ul>
<div class="section" id="proof">
<h3>Proof<a class="headerlink" href="#proof" title="Permalink to this headline">¶</a></h3>
<p><strong>Sum of normally distributed random variables.</strong></p>
<p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent random variables that are normally distributed (and therefore also jointly so), then their sum is also normally distributed. i.e., <span class="math notranslate nohighlight">\(Z=X+Y\)</span> is normally distributed with its mean being the sum of the two means, and its variance being the sum of the two variances.</p>
<p>Consider the linear model and define the <span class="math notranslate nohighlight">\(N \times H\)</span> design matrix <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> with elements</p>
<div class="math notranslate nohighlight">
\[
R_{nh} \equiv \phi^{(h)} \left( \boldsymbol{x}^{(n)} \right).
\]</div>
<p>Then <span class="math notranslate nohighlight">\(\boldsymbol{y}_N = \boldsymbol{R} \boldsymbol{\theta}\)</span> is the vector of model predictions, i.e.</p>
<div class="math notranslate nohighlight">
\[
y^{(n)} = \sum_{h=1}^H R_{nh} \boldsymbol{\theta^{(h)}}.
\]</div>
<p>Assume that we have a Gaussian prior for the linear model weights <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> with zero mean and a diagonal covariance matrix</p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{\theta}) = \mathcal{N} \left( \boldsymbol{\theta}; 0, \sigma_\theta^2 \boldsymbol{I} \right).
\]</div>
<p>Now, since <span class="math notranslate nohighlight">\(y\)</span> is a linear function of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, it is also Gaussian distributed with mean zero. Its covariance matrix becomes</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{Q} = \langle \boldsymbol{y} \boldsymbol{y}^T \rangle = \langle \boldsymbol{R} \boldsymbol{\theta} \boldsymbol{\theta}^T \boldsymbol{R}^T \rangle
= \sigma_\theta^2 \boldsymbol{R} \boldsymbol{R}^T,
\]</div>
<p>which implies that</p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{y}) = \mathcal{N} \left( \boldsymbol{y}; 0, \sigma_\theta^2 \boldsymbol{R} \boldsymbol{R}^T \right).
\]</div>
<p>This will be true for any set of points <span class="math notranslate nohighlight">\(\boldsymbol{X}_N\)</span>; which is the defining property of a <strong>Gaussian process</strong>.</p>
<ul class="simple">
<li><p>What about the target values <span class="math notranslate nohighlight">\(\boldsymbol{t}\)</span>?</p></li>
</ul>
<p>Well, if <span class="math notranslate nohighlight">\(t^{(n)}\)</span> is assumed to differ by additive Gaussian noise, i.e.,</p>
<div class="math notranslate nohighlight">
\[
t^{(n)} = y^{(n)} + \varepsilon^{(n)},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\varepsilon^{(n)} \sim \mathcal{N} \left( 0, \sigma_\nu^2 \right)\)</span>; then <span class="math notranslate nohighlight">\(\boldsymbol{t}\)</span> also has a Gaussian prior distribution</p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{t}) = \mathcal{N} \left( \boldsymbol{t}; 0, \boldsymbol{C} \right),
\]</div>
<p>where the covariance matrix of this target distribution is given by</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{C} = \boldsymbol{Q} + \sigma_\nu^2 \boldsymbol{I} = \sigma_\theta^2 \boldsymbol{R} \boldsymbol{R}^T + \sigma_\nu^2 \boldsymbol{I}.
\]</div>
</div>
<div class="section" id="the-covariance-matrix-as-the-central-object">
<h3>The covariance matrix as the central object<a class="headerlink" href="#the-covariance-matrix-as-the-central-object" title="Permalink to this headline">¶</a></h3>
<p>The covariance matrices are given by</p>
<div class="math notranslate nohighlight">
\[
Q_{nn'} = \sigma_\theta^2 \sum_h \phi^{(h)} \left( \boldsymbol{x}^{(n)} \right) \phi^{(h)} \left( \boldsymbol{x}^{(n')} \right),
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
C_{nn'} = Q_{nn'} + \delta_{nn'} \sigma_\nu^2.
\]</div>
<p>This means that the correlation between target values <span class="math notranslate nohighlight">\(t^{(n)}\)</span> and <span class="math notranslate nohighlight">\(t^{(n')}\)</span> is determined by the points <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n')}\)</span> and the behaviour of the basis functions.</p>
</div>
</div>
<div class="section" id="non-parametric-approach-mean-and-covariance-functions">
<h2>Non-parametric approach: Mean and covariance functions<a class="headerlink" href="#non-parametric-approach-mean-and-covariance-functions" title="Permalink to this headline">¶</a></h2>
<p>In fact, we don’t really need the basis functions and their parameters anymore. The influence of these appear only in the covariance matrix that describes the distribution of the targets, which is our key object. We can replace the parametric model altogether with a <strong>covariance function</strong> <span class="math notranslate nohighlight">\(C( \boldsymbol{x}, \boldsymbol{x}' )\)</span> which generates the  elements of the covariance matrix</p>
<div class="math notranslate nohighlight">
\[
Q_{nn'} = C \left( \boldsymbol{x}^{(n)}, \boldsymbol{x}^{(n')} \right),
\]</div>
<p>for any set of points <span class="math notranslate nohighlight">\(\left\{ \boldsymbol{x}^{(n)} \right\}_{n=1}^N\)</span>.</p>
<p>Note, however, that <span class="math notranslate nohighlight">\(\boldsymbol{Q}\)</span> must be positive-definite. This constrains the set of valid covariance functions.</p>
<p>Once we have defined a covariance function, the covariance matrix for the target values will be given by</p>
<div class="math notranslate nohighlight">
\[
C_{nn'} = C \left( \boldsymbol{x}^{(n)}, \boldsymbol{x}^{(n')} \right) + \sigma_\nu^2 \delta_{nn'}.
\]</div>
<p>A wide range of different covariance contributions can be <a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_process#Covariance_functions">constructed</a>. These standard covariance functions are typically parametrized with hyperparameters <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}\)</span> so that</p>
<div class="math notranslate nohighlight">
\[
C_{nn'} = C \left( \boldsymbol{x}^{(n)}, \boldsymbol{x}^{(n')}, \boldsymbol{\alpha} \right) + \delta_{nn'} \Delta \left( \boldsymbol{x}^{(n)};  \boldsymbol{\alpha} \right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Delta\)</span> is usually included as a flexible noise model.</p>
<div class="section" id="stationary-kernels">
<h3>Stationary kernels<a class="headerlink" href="#stationary-kernels" title="Permalink to this headline">¶</a></h3>
<p>The most common types of covariance functions are stationary, or translationally invariant, which implies that</p>
<div class="math notranslate nohighlight">
\[
C \left( \boldsymbol{x}^{(n)}, \boldsymbol{x}^{(n')}, \boldsymbol{\alpha} \right) = D \left( \boldsymbol{x} - \boldsymbol{x}'; \boldsymbol{\alpha} \right),
\]</div>
<p>where the function <span class="math notranslate nohighlight">\(D\)</span> is often referred to as a <em>kernel</em>.</p>
<p>A very standard kernel is the RBF (also known as Exponentiated Quadratic or Gaussian kernel) which is differentiable infinitely many times (hence, very smooth),</p>
<div class="math notranslate nohighlight">
\[
C_\mathrm{RBF}(\mathbf{x},\mathbf{x}'; \boldsymbol{\alpha}) = \alpha_0 + \alpha_1 \exp \left[ -\frac{1}{2} \sum_{i=1}^I \frac{(x_{i} - x_{i}')^2}{r_i^2} \right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(I\)</span> denotes the dimensionality of the input space. The hyperparameters are: <span class="math notranslate nohighlight">\(\{ \alpha_0, \alpha_1, \vec{r} \}\)</span>. Sometimes, a single correlation length <span class="math notranslate nohighlight">\(r\)</span> is used.</p>
</div>
</div>
<div class="section" id="gp-models-for-regression">
<h2>GP models for regression<a class="headerlink" href="#gp-models-for-regression" title="Permalink to this headline">¶</a></h2>
<p>Let us return to the problem of predicting <span class="math notranslate nohighlight">\(t^{(N+1)}\)</span> given <span class="math notranslate nohighlight">\(\boldsymbol{t}_N\)</span>. The independent variables <span class="math notranslate nohighlight">\(\boldsymbol{X}_{N+1}\)</span> are also given, but will be omitted from the conditional pdfs below.</p>
<p>The joint density is</p>
<div class="math notranslate nohighlight">
\[
p \left( t^{(N+1)}, \boldsymbol{t}_N \right) = p \left( t^{(N+1)} | \boldsymbol{t}_N \right) p \left( \boldsymbol{t}_N \right) 
\quad \Rightarrow \quad
p \left( t^{(N+1)} | \boldsymbol{t}_N \right) = \frac{p \left( t^{(N+1)}, \boldsymbol{t}_N \right)}{p \left( \boldsymbol{t}_N \right) }.
\]</div>
<p>Since both <span class="math notranslate nohighlight">\(p \left( t^{(N+1)}, \boldsymbol{t}_N \right)\)</span> and <span class="math notranslate nohighlight">\(p \left( \boldsymbol{t}_N \right)\)</span> are Gaussian distributions, then the conditional distribution, obtained by the ratio, must also be a Gaussian. Let us use the notation <span class="math notranslate nohighlight">\(\boldsymbol{C}_{N+1}\)</span> for the <span class="math notranslate nohighlight">\((N+1) \times (N+1)\)</span> covariance matrix for <span class="math notranslate nohighlight">\(\boldsymbol{t}_{N+1} = \left( \boldsymbol{t}_N, t^{(N+1)} \right)\)</span>. This implies that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
p \left( t^{(N+1)} | \boldsymbol{t}_N \right) \propto \exp \left[ -\frac{1}{2} \left( \boldsymbol{t}_N, t^{(N+1)} \right) \boldsymbol{C}_{N+1}^{-1} 
\begin{pmatrix}
\boldsymbol{t}_N \\
t^{(N+1)}
\end{pmatrix}
\right]
\end{split}\]</div>
<p><strong>Summary.</strong></p>
<p>The prediction of the (Gaussian) pdf for <span class="math notranslate nohighlight">\(t^{(N+1)}\)</span> requires an inversion of the covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{C}_{N+1}^{-1}\)</span>.</p>
<div class="section" id="elegant-approach-using-linear-algebra-tricks">
<h3>Elegant approach using linear algebra tricks<a class="headerlink" href="#elegant-approach-using-linear-algebra-tricks" title="Permalink to this headline">¶</a></h3>
<p>Let us split the <span class="math notranslate nohighlight">\(\boldsymbol{C}_{N+1}\)</span> covariance matrix into four different blocks</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{C}_{N+1} =
\begin{pmatrix}
\boldsymbol{C}_N &amp; \boldsymbol{k} \\
\boldsymbol{k}^T &amp; \kappa
\end{pmatrix},
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{C}_N\)</span> is the <span class="math notranslate nohighlight">\(N \times N\)</span> covariance matrix (which depends on the positions <span class="math notranslate nohighlight">\(\boldsymbol{X}_N\)</span>), <span class="math notranslate nohighlight">\(\boldsymbol{k}\)</span> is an <span class="math notranslate nohighlight">\(N \times 1\)</span> vector (that describes the covariance of <span class="math notranslate nohighlight">\(\boldsymbol{X}_N\)</span> with <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(N+1)}\)</span>), while <span class="math notranslate nohighlight">\(\kappa\)</span> is the single diagonal element obtained from <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(N+1)}\)</span>.</p>
<p>We can use the partitioned inverse equations (Barnett, 1979) to rewrite <span class="math notranslate nohighlight">\(\boldsymbol{C}_{N+1}^{-1}\)</span> in terms of <span class="math notranslate nohighlight">\(\boldsymbol{C}_{N}^{-1}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{C}_{N}\)</span> as follows</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{C}_{N+1}^{-1} =
\begin{pmatrix}
\boldsymbol{M}_N &amp; \boldsymbol{m} \\
\boldsymbol{m}^T &amp; \mu
\end{pmatrix},
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mu &amp;= \left( \kappa - \boldsymbol{k}^T \boldsymbol{C}_N^{-1} \boldsymbol{k} \right)^{-1} \\
\boldsymbol{m} &amp;= -\mu \boldsymbol{C}_N^{-1} \boldsymbol{k} \\
\boldsymbol{M}_N &amp;= \boldsymbol{C}_N^{-1} + \frac{1}{\mu} \boldsymbol{m} \boldsymbol{m}^T.
\end{align*}
\end{split}\]</div>
<p><strong>Question.</strong></p>
<p>What are the dimensions of the different blocks? Check that the answer.</p>
<p>This implies that we can make a prediction for the Gaussian pdf of <span class="math notranslate nohighlight">\(t^{(N+1)}\)</span> (meaning that we predict its value with an associated uncertainty) for an <span class="math notranslate nohighlight">\(N^3\)</span> computational cost (the inversion of an <span class="math notranslate nohighlight">\(N \times N\)</span> matrix).</p>
<p><strong>Summary.</strong></p>
<p>The prediction for <span class="math notranslate nohighlight">\(t^{(N+1)}\)</span> is a Gaussian</p>
<div class="math notranslate nohighlight">
\[
p \left( t^{(N+1)} | \boldsymbol{t}_N \right) = \frac{1}{Z} \exp
\left[
-\frac{\left( t^{(N+1)} - \hat{t}^{(N+1)} \right)^2}{2 \sigma_{\hat{t}_{N+1}}^2}
\right]
\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathrm{mean:} &amp; \quad \hat{t}^{(N+1)} = \boldsymbol{k}^T \boldsymbol{C}_N^{-1} \boldsymbol{t}_N \\
\mathrm{variance:} &amp; \quad \sigma_{\hat{t}_{N+1}}^2 = \kappa - \boldsymbol{k}^T \boldsymbol{C}_N^{-1} \boldsymbol{k}.
\end{align*}
\end{split}\]</div>
<p>In fact, since the prediction only depends on the <span class="math notranslate nohighlight">\(N\)</span> available data we might as well predict several new target values at once. Consider <span class="math notranslate nohighlight">\(\boldsymbol{t}_M = \{ t^{(N+i)} \}_{i=1}^M\)</span> so that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{C}_{N+M} =
\begin{pmatrix}
\boldsymbol{C}_N &amp; \boldsymbol{k} \\
\boldsymbol{k}^T &amp; \boldsymbol{\kappa}
\end{pmatrix},
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{k}\)</span> is now an <span class="math notranslate nohighlight">\(N \times M\)</span> matrix and <span class="math notranslate nohighlight">\(\boldsymbol{\kappa}\)</span> an <span class="math notranslate nohighlight">\(M \times M\)</span> matrix.</p>
<p>The prediction becomes a multivariate Gaussian</p>
<div class="math notranslate nohighlight">
\[
p \left( \boldsymbol{t}_{N+M} | \boldsymbol{t}_N \right) = \frac{1}{Z} \exp
\left[
-\frac{1}{2} \left( \boldsymbol{t}_M - \hat{\boldsymbol{t}}_M \right)^T \boldsymbol{\Sigma}_M^{-1} \left( \boldsymbol{t}_M - \hat{\boldsymbol{t}}_M \right)
\right],
\]</div>
<p>where the <span class="math notranslate nohighlight">\(M \times 1\)</span> mean vector and <span class="math notranslate nohighlight">\(M \times M\)</span> covariance matrix are</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{\boldsymbol{t}}_M &amp;= \boldsymbol{k}^T \boldsymbol{C}_N^{-1} \boldsymbol{t}_N \\
\boldsymbol{\Sigma}_M &amp;= \boldsymbol{\kappa} - \boldsymbol{k}^T \boldsymbol{C}_N^{-1} \boldsymbol{k}.
\end{align*}
\end{split}\]</div>
</div>
<div class="section" id="optimizing-the-gp-model-hyperparameters">
<h3>Optimizing the GP model hyperparameters<a class="headerlink" href="#optimizing-the-gp-model-hyperparameters" title="Permalink to this headline">¶</a></h3>
<p>Predictions can be made once we have</p>
<ol class="simple">
<li><p>Chosen an appropriate covariance function.</p></li>
<li><p>Determined the hyperparameters.</p></li>
<li><p>Evaluated the relevant blocks in the covariance function and inverted <span class="math notranslate nohighlight">\(\\boldsymbol{C}_N\)</span>.</p></li>
</ol>
<p>How do we determine the hyperparameters <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}\)</span>? Well, recall that</p>
<div class="math notranslate nohighlight">
\[
p \left( \boldsymbol{t}_N \right) = \frac{1}{Z} \exp \left[ -\frac{1}{2} \boldsymbol{t}_N^T \boldsymbol{C}_{N}^{-1} \boldsymbol{t}_N 
\right].
\]</div>
<p>This pdf is basically a data likelihood.</p>
<ul class="simple">
<li><p>The frequentist approach would be to find the set of hyperparameters <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}^*\)</span> that maximizes the data likelihood, i.e. that minimizes <span class="math notranslate nohighlight">\(\boldsymbol{t}_N^T \boldsymbol{C}_{N}^{-1} \boldsymbol{t}_N\)</span>.</p></li>
<li><p>A Bayesian approach would be to assign a prior to the hyperparameters and seek a posterior pdf <span class="math notranslate nohighlight">\(p(\boldsymbol{\alpha} | \boldsymbol{t}_N)\)</span> instead.</p></li>
</ul>
<p>The former approach is absolutely dominating the literature on GP regression. The covariance function hyperparameters are first optimized and then used for regression.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "furnstahl/Physics-8820",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks/Gaussian_processes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="demo-GaussianProcesses.html" title="previous page"><span class="section-number">7.1. </span>Gaussian processes demonstration</a>
    <a class='right-next' id="next-link" href="Gaussian_processes_exercises.html" title="next page"><span class="section-number">7.3. </span>Exercise: Gaussian Process models with GPy</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dick Furnstahl<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>